{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b1063252-1d71-4596-a3ec-ea1aba70d488",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "caca88ed-66f9-42a6-b6bd-ad4aa260d686",
   "metadata": {},
   "outputs": [],
   "source": [
    "#read all the words\n",
    "words = open('names.txt', 'r').read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "92d5bb72-78db-4adb-9a94-76a8d6e2c1a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32033"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "850146f1-a75d-4aa3-834e-b4dd55d97a96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z', 0: '.'}\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(list(set(''.join(words))))\n",
    "stoi = {s:i+1 for i, s in enumerate(chars)}\n",
    "stoi['.'] = 0\n",
    "itos = {i:s for s, i in stoi.items()}\n",
    "print(itos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "0f6147aa-4356-4d41-953f-7c6ee55d7268",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the dataset\n",
    "\n",
    "block_size = 3 #context length\n",
    "X, Y = [], []\n",
    "for w in words:\n",
    "    # print(w)\n",
    "    context = [0] * block_size #starts as block_size #'.'s. (Since itos[0] is . and we use the integer representation of the character)\n",
    "    for ch in w + '.':\n",
    "        ix = stoi[ch]\n",
    "        X.append(context)\n",
    "        Y.append(ix)\n",
    "        # print(''.join(itos[i] for i in context), '---->', itos[ix])\n",
    "        context = context[1:] + [ix]\n",
    "\n",
    "X = torch.tensor(X)\n",
    "Y = torch.tensor(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "88e8e09f-0e44-4abc-8769-99abe73a93f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([228146, 3]), torch.int64, torch.Size([228146]), torch.int64)"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, X.dtype, Y.shape, Y.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "9706201d-0a12-496d-8c25-b73b7ce5f892",
   "metadata": {},
   "outputs": [],
   "source": [
    "C = torch.randn((27,2)) #27 possible characters, each with 2-dimensional embedding. C is our lookup table of vector embedding for vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6babe81a-3452-476e-875a-f38618206a8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1.8775, -0.5638])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "46f0f8c2-abd5-4cf4-b8a8-3e87f044d27b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1.5116,  2.3201])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.one_hot(torch.tensor(5), num_classes= 27).float() @ C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "2f9ab664-2344-49a3-88d5-a07e1b51307e",
   "metadata": {},
   "outputs": [],
   "source": [
    "emb = C[X]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "0fd390e8-037f-4c85-bd3f-c8bd50251d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "W1 = torch.randn((6, 100))\n",
    "b1 = torch.randn(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "b20adb17-efd3-4a30-acb5-c7a0e2877b5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.2012, -0.0892,  0.2012, -0.0892,  0.2012, -0.0892],\n",
       "        [ 0.2012, -0.0892,  0.2012, -0.0892, -1.5116,  2.3201],\n",
       "        [ 0.2012, -0.0892, -1.5116,  2.3201,  1.8382,  0.2528],\n",
       "        [-1.5116,  2.3201,  1.8382,  0.2528,  1.8382,  0.2528],\n",
       "        [ 1.8382,  0.2528,  1.8382,  0.2528, -0.8278,  0.0196],\n",
       "        [ 0.2012, -0.0892,  0.2012, -0.0892,  0.2012, -0.0892],\n",
       "        [ 0.2012, -0.0892,  0.2012, -0.0892,  0.7286,  1.2661],\n",
       "        [ 0.2012, -0.0892,  0.7286,  1.2661, -0.2472,  0.9470],\n",
       "        [ 0.7286,  1.2661, -0.2472,  0.9470,  0.4170, -0.1270],\n",
       "        [-0.2472,  0.9470,  0.4170, -0.1270,  0.1409, -0.1828],\n",
       "        [ 0.4170, -0.1270,  0.1409, -0.1828,  0.4170, -0.1270],\n",
       "        [ 0.1409, -0.1828,  0.4170, -0.1270, -0.8278,  0.0196],\n",
       "        [ 0.2012, -0.0892,  0.2012, -0.0892,  0.2012, -0.0892],\n",
       "        [ 0.2012, -0.0892,  0.2012, -0.0892, -0.8278,  0.0196],\n",
       "        [ 0.2012, -0.0892, -0.8278,  0.0196,  0.1409, -0.1828],\n",
       "        [-0.8278,  0.0196,  0.1409, -0.1828, -0.8278,  0.0196],\n",
       "        [ 0.2012, -0.0892,  0.2012, -0.0892,  0.2012, -0.0892],\n",
       "        [ 0.2012, -0.0892,  0.2012, -0.0892,  0.4170, -0.1270],\n",
       "        [ 0.2012, -0.0892,  0.4170, -0.1270, -0.6830, -1.0321],\n",
       "        [ 0.4170, -0.1270, -0.6830, -1.0321, -0.8278,  0.0196],\n",
       "        [-0.6830, -1.0321, -0.8278,  0.0196,  1.2662,  0.5424],\n",
       "        [-0.8278,  0.0196,  1.2662,  0.5424, -1.5116,  2.3201],\n",
       "        [ 1.2662,  0.5424, -1.5116,  2.3201, -0.2472,  0.9470],\n",
       "        [-1.5116,  2.3201, -0.2472,  0.9470, -0.2472,  0.9470],\n",
       "        [-0.2472,  0.9470, -0.2472,  0.9470, -0.8278,  0.0196],\n",
       "        [ 0.2012, -0.0892,  0.2012, -0.0892,  0.2012, -0.0892],\n",
       "        [ 0.2012, -0.0892,  0.2012, -0.0892, -0.6830, -1.0321],\n",
       "        [ 0.2012, -0.0892, -0.6830, -1.0321,  0.7286,  1.2661],\n",
       "        [-0.6830, -1.0321,  0.7286,  1.2661, -1.0922,  1.4000],\n",
       "        [ 0.7286,  1.2661, -1.0922,  1.4000, -0.8882, -1.4069],\n",
       "        [-1.0922,  1.4000, -0.8882, -1.4069,  0.4170, -0.1270],\n",
       "        [-0.8882, -1.4069,  0.4170, -0.1270, -0.8278,  0.0196]])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cat([emb[:, 0, :], emb[:, 1, :], emb[:, 2, :]], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "71553ac3-53d0-46dc-98a8-303cc6582208",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.0731,  1.2645,  1.0731,  1.2645,  1.0731,  1.2645],\n",
       "        [ 1.0731,  1.2645,  1.0731,  1.2645, -1.8775, -0.5638],\n",
       "        [ 1.0731,  1.2645, -1.8775, -0.5638,  0.0778, -0.6400],\n",
       "        [-1.8775, -0.5638,  0.0778, -0.6400,  0.0778, -0.6400],\n",
       "        [ 0.0778, -0.6400,  0.0778, -0.6400,  0.7460,  1.4331],\n",
       "        [ 1.0731,  1.2645,  1.0731,  1.2645,  1.0731,  1.2645],\n",
       "        [ 1.0731,  1.2645,  1.0731,  1.2645,  0.1652, -0.9800],\n",
       "        [ 1.0731,  1.2645,  0.1652, -0.9800,  0.0865,  1.1077],\n",
       "        [ 0.1652, -0.9800,  0.0865,  1.1077, -0.1222, -0.3618],\n",
       "        [ 0.0865,  1.1077, -0.1222, -0.3618, -0.5252,  0.5518],\n",
       "        [-0.1222, -0.3618, -0.5252,  0.5518, -0.1222, -0.3618],\n",
       "        [-0.5252,  0.5518, -0.1222, -0.3618,  0.7460,  1.4331],\n",
       "        [ 1.0731,  1.2645,  1.0731,  1.2645,  1.0731,  1.2645],\n",
       "        [ 1.0731,  1.2645,  1.0731,  1.2645,  0.7460,  1.4331],\n",
       "        [ 1.0731,  1.2645,  0.7460,  1.4331, -0.5252,  0.5518],\n",
       "        [ 0.7460,  1.4331, -0.5252,  0.5518,  0.7460,  1.4331],\n",
       "        [ 1.0731,  1.2645,  1.0731,  1.2645,  1.0731,  1.2645],\n",
       "        [ 1.0731,  1.2645,  1.0731,  1.2645, -0.1222, -0.3618],\n",
       "        [ 1.0731,  1.2645, -0.1222, -0.3618,  1.0459,  1.2646],\n",
       "        [-0.1222, -0.3618,  1.0459,  1.2646,  0.7460,  1.4331],\n",
       "        [ 1.0459,  1.2646,  0.7460,  1.4331,  0.6760, -1.5680],\n",
       "        [ 0.7460,  1.4331,  0.6760, -1.5680, -1.8775, -0.5638],\n",
       "        [ 0.6760, -1.5680, -1.8775, -0.5638,  0.0865,  1.1077],\n",
       "        [-1.8775, -0.5638,  0.0865,  1.1077,  0.0865,  1.1077],\n",
       "        [ 0.0865,  1.1077,  0.0865,  1.1077,  0.7460,  1.4331],\n",
       "        [ 1.0731,  1.2645,  1.0731,  1.2645,  1.0731,  1.2645],\n",
       "        [ 1.0731,  1.2645,  1.0731,  1.2645,  1.0459,  1.2646],\n",
       "        [ 1.0731,  1.2645,  1.0459,  1.2646,  0.1652, -0.9800],\n",
       "        [ 1.0459,  1.2646,  0.1652, -0.9800,  0.0476, -1.6228],\n",
       "        [ 0.1652, -0.9800,  0.0476, -1.6228, -2.4654,  0.9316],\n",
       "        [ 0.0476, -1.6228, -2.4654,  0.9316, -0.1222, -0.3618],\n",
       "        [-2.4654,  0.9316, -0.1222, -0.3618,  0.7460,  1.4331]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cat(torch.unbind(emb, 1), 1) #dynamic but inefficient (creates a new tensor meaning more memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "908589a7-8681-4bcf-a2e9-f2efaa0ddd6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nNOTE ON TORCH.TENSOR INTERNALS:\\nTensors have an internal sotrage which is all of the data in memory as a one dimensional vector. Whenever we manipulate the tensor with a function\\nlike view(), nothing in the tensors storage changes, just some of the attirbutes of the tensor change which changes how the values in storage are interpreted\\nor should be represented. No memory or values are changed or moved, only some attributes which dictate the values' representation/interpretation.\\nThat's why tensor.view() is great because no memory is being changed.\\n\""
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb.view(32, 6) #BEST \n",
    "\"\"\"\n",
    "NOTE ON TORCH.TENSOR INTERNALS:\n",
    "Tensors have an internal sotrage which is all of the data in memory as a one dimensional vector. Whenever we manipulate the tensor with a function\n",
    "like view(), nothing in the tensors storage changes, just some of the attirbutes of the tensor change which changes how the values in storage are interpreted\n",
    "or should be represented. No memory or values are changed or moved, only some attributes which dictate the values' representation/interpretation.\n",
    "That's why tensor.view() is great because no memory is being changed.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "07d797ad-1213-48d6-aa73-b7bee371a6fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "h = torch.tanh(emb.view(emb.shape[0], 6) @ W1 + b1) # first layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "3213e7d8-22a9-47a6-8a77-a47a473fe593",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.8648, -0.8292, -0.0760,  ..., -0.9338,  0.9306,  0.6233],\n",
       "        [ 0.4545, -0.9312,  0.2247,  ..., -0.9998, -0.8601, -0.5025],\n",
       "        [-1.0000,  0.9998,  1.0000,  ..., -0.9999, -0.9325,  0.9612],\n",
       "        ...,\n",
       "        [-0.9572,  0.9993,  0.8370,  ..., -0.9968, -0.9918,  0.9913],\n",
       "        [ 0.9622,  0.2132,  1.0000,  ...,  0.2152,  0.7130, -0.9966],\n",
       "        [ 0.1329, -1.0000, -0.7810,  ..., -0.9998,  0.9750,  0.7462]])"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "ac087dbd-0494-4c42-88e2-fb7ae2467b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "W2 = torch.randn((100, 27))\n",
    "b2 = torch.randn(27)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "621ea89e-9c75-4eb5-8963-dbf7b87e914f",
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = h @ W2 + b2 #second linear layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "8b712a3f-7ce5-4db9-bfc7-a87ad10350a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 27])"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "f5f2b30e-6cfb-418e-85e5-c71f768321ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = logits.exp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "5979bca0-e45c-48d6-bb3f-507bf8fcf1cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[4.7008e-09, 8.8714e-06, 3.3168e-01, 1.2584e-05, 2.3167e-09, 1.2796e-10,\n",
       "         8.3215e-11, 6.4294e-07, 5.1951e-08, 3.3789e-06, 1.5189e-03, 6.8262e-05,\n",
       "         3.7613e-07, 6.2532e-13, 1.5927e-10, 8.2239e-06, 1.8323e-08, 1.4269e-08,\n",
       "         2.6168e-05, 8.2330e-07, 6.5794e-01, 1.8119e-11, 4.5998e-09, 2.0341e-05,\n",
       "         5.4432e-06, 8.7054e-03, 6.0415e-12],\n",
       "        [9.3150e-01, 2.4325e-06, 1.0237e-09, 1.7496e-06, 6.6780e-09, 6.4393e-08,\n",
       "         5.7660e-08, 6.3291e-08, 2.0872e-02, 2.4175e-03, 6.2874e-04, 8.8956e-05,\n",
       "         8.9498e-07, 2.4573e-08, 3.1046e-13, 1.2666e-05, 9.7728e-11, 2.0570e-06,\n",
       "         3.1907e-02, 6.4657e-09, 1.2503e-02, 1.6720e-08, 5.3784e-05, 6.9911e-06,\n",
       "         3.7002e-09, 1.9793e-06, 1.6624e-06],\n",
       "        [7.0282e-11, 1.8660e-11, 7.7693e-04, 2.1158e-04, 1.2086e-09, 2.1356e-12,\n",
       "         1.8347e-05, 2.8229e-01, 3.1247e-06, 2.9106e-08, 1.4384e-09, 3.9568e-05,\n",
       "         4.3158e-04, 1.9857e-14, 7.1619e-01, 1.3114e-06, 1.8246e-09, 1.2738e-07,\n",
       "         2.9759e-09, 1.6988e-05, 8.3533e-08, 1.6599e-12, 7.9265e-08, 1.5408e-07,\n",
       "         4.3171e-06, 1.5859e-05, 2.4544e-06],\n",
       "        [5.0087e-13, 6.2196e-10, 1.0000e+00, 1.9371e-08, 3.1239e-14, 4.7165e-12,\n",
       "         1.2845e-20, 2.4907e-12, 1.4880e-11, 4.1866e-12, 1.2496e-15, 1.1546e-10,\n",
       "         7.5164e-18, 6.1488e-19, 2.8422e-12, 1.4389e-11, 1.6299e-17, 4.6190e-18,\n",
       "         3.1794e-14, 1.7401e-08, 1.0804e-13, 2.5585e-14, 4.3413e-14, 7.0407e-11,\n",
       "         5.1768e-12, 7.2442e-11, 4.9768e-11],\n",
       "        [8.5623e-13, 1.9366e-09, 5.9307e-12, 2.7144e-13, 6.0188e-13, 1.2119e-10,\n",
       "         1.3140e-16, 2.0468e-13, 4.1894e-09, 9.3742e-01, 4.4442e-09, 7.6510e-10,\n",
       "         1.9795e-10, 1.6601e-11, 1.5308e-11, 2.0601e-07, 8.6963e-15, 1.7767e-11,\n",
       "         5.3877e-02, 7.3348e-10, 8.7016e-03, 2.2181e-11, 1.0080e-15, 3.4421e-12,\n",
       "         7.7274e-15, 5.7440e-10, 5.5566e-17],\n",
       "        [4.7008e-09, 8.8714e-06, 3.3168e-01, 1.2584e-05, 2.3167e-09, 1.2796e-10,\n",
       "         8.3215e-11, 6.4294e-07, 5.1951e-08, 3.3789e-06, 1.5189e-03, 6.8262e-05,\n",
       "         3.7613e-07, 6.2532e-13, 1.5927e-10, 8.2239e-06, 1.8323e-08, 1.4269e-08,\n",
       "         2.6168e-05, 8.2330e-07, 6.5794e-01, 1.8119e-11, 4.5998e-09, 2.0341e-05,\n",
       "         5.4432e-06, 8.7054e-03, 6.0415e-12],\n",
       "        [4.0526e-09, 7.2408e-08, 1.9883e-02, 9.6227e-06, 7.1508e-11, 7.2158e-09,\n",
       "         3.2481e-11, 6.2197e-11, 1.2649e-06, 2.6281e-10, 3.5248e-10, 5.1517e-01,\n",
       "         1.1673e-07, 5.4407e-11, 5.0176e-10, 1.1887e-05, 1.8415e-17, 1.6122e-13,\n",
       "         1.5841e-06, 9.8372e-05, 4.6482e-01, 5.6479e-16, 2.0217e-11, 4.1631e-08,\n",
       "         1.2091e-09, 1.8133e-08, 2.3035e-09],\n",
       "        [2.9790e-09, 2.7003e-07, 2.5562e-04, 6.9822e-06, 1.9080e-06, 3.9110e-08,\n",
       "         2.0318e-10, 7.9455e-04, 5.9020e-03, 7.1976e-03, 1.5695e-01, 2.0589e-04,\n",
       "         8.4742e-09, 8.5056e-11, 1.0040e-07, 3.4775e-02, 1.6807e-15, 3.8954e-08,\n",
       "         5.7328e-01, 2.1567e-01, 2.8114e-03, 1.4581e-11, 6.1239e-08, 1.7931e-03,\n",
       "         6.0098e-10, 3.5703e-04, 6.5131e-09],\n",
       "        [6.4766e-11, 7.0621e-09, 8.9781e-08, 3.5049e-09, 4.4253e-14, 5.4687e-14,\n",
       "         2.0546e-11, 3.6965e-07, 2.0614e-06, 8.2708e-06, 2.2119e-08, 2.0197e-06,\n",
       "         4.7402e-11, 3.2283e-15, 3.7686e-09, 5.8010e-08, 3.1111e-13, 4.7529e-13,\n",
       "         3.0663e-06, 9.9985e-01, 1.3753e-04, 7.2909e-12, 2.3370e-09, 3.4528e-07,\n",
       "         5.3354e-09, 3.0526e-09, 3.4641e-15],\n",
       "        [8.8491e-09, 6.4012e-04, 9.7117e-01, 1.5167e-06, 7.2415e-09, 4.9691e-09,\n",
       "         1.1874e-10, 7.4823e-09, 8.3151e-08, 1.4973e-07, 1.5917e-06, 2.7969e-06,\n",
       "         3.2274e-12, 2.2100e-13, 1.6632e-09, 4.6701e-04, 9.1219e-11, 4.9956e-11,\n",
       "         2.3317e-04, 2.1825e-02, 5.4204e-06, 9.9486e-08, 4.8420e-05, 6.1141e-05,\n",
       "         9.7277e-08, 5.5412e-03, 1.3076e-09],\n",
       "        [1.4597e-10, 2.1407e-06, 7.7995e-02, 1.9996e-06, 1.8817e-11, 4.2761e-12,\n",
       "         6.1124e-12, 5.8404e-08, 4.6475e-10, 4.5157e-07, 9.3106e-06, 6.8402e-05,\n",
       "         1.1070e-06, 4.0494e-14, 1.4224e-11, 1.9345e-07, 1.0369e-08, 1.2943e-09,\n",
       "         1.4012e-06, 2.0321e-08, 9.2118e-01, 1.5199e-12, 2.9353e-10, 1.6769e-07,\n",
       "         6.1354e-07, 7.3532e-04, 1.1407e-13],\n",
       "        [1.3090e-08, 1.3687e-07, 7.2423e-10, 2.0715e-11, 7.0919e-08, 4.6587e-13,\n",
       "         5.6987e-13, 7.5136e-12, 2.9000e-09, 3.7937e-06, 9.9995e-01, 3.1182e-12,\n",
       "         1.9419e-12, 2.7429e-13, 1.5492e-13, 9.0790e-10, 5.3882e-10, 9.4927e-08,\n",
       "         6.6041e-06, 2.9364e-11, 3.6211e-05, 3.1393e-11, 3.4862e-11, 8.5922e-08,\n",
       "         2.0933e-13, 2.1028e-06, 4.4924e-14],\n",
       "        [4.7008e-09, 8.8714e-06, 3.3168e-01, 1.2584e-05, 2.3167e-09, 1.2796e-10,\n",
       "         8.3215e-11, 6.4294e-07, 5.1951e-08, 3.3789e-06, 1.5189e-03, 6.8262e-05,\n",
       "         3.7613e-07, 6.2532e-13, 1.5927e-10, 8.2239e-06, 1.8323e-08, 1.4269e-08,\n",
       "         2.6168e-05, 8.2330e-07, 6.5794e-01, 1.8119e-11, 4.5998e-09, 2.0341e-05,\n",
       "         5.4432e-06, 8.7054e-03, 6.0415e-12],\n",
       "        [5.0837e-09, 1.8883e-07, 9.5590e-11, 1.1155e-11, 1.8393e-08, 1.9428e-13,\n",
       "         1.5828e-11, 1.5210e-11, 2.4523e-09, 2.8805e-07, 9.9994e-01, 6.0302e-12,\n",
       "         2.4906e-12, 2.4641e-14, 4.9577e-14, 2.0955e-10, 2.0225e-09, 3.3248e-08,\n",
       "         9.3069e-06, 3.3443e-10, 4.6728e-05, 6.8157e-11, 2.6627e-10, 1.4124e-07,\n",
       "         2.7298e-13, 1.1544e-06, 1.5890e-14],\n",
       "        [1.4391e-04, 5.4176e-06, 1.1209e-04, 9.5366e-06, 1.9701e-06, 1.3308e-11,\n",
       "         4.5262e-04, 4.3207e-05, 8.6348e-06, 6.6981e-10, 9.2055e-01, 4.8107e-05,\n",
       "         7.1754e-05, 3.6904e-14, 1.0244e-06, 1.0351e-07, 1.7316e-02, 3.2828e-06,\n",
       "         1.6930e-07, 7.4513e-05, 5.8243e-02, 2.1262e-10, 7.3790e-04, 2.0280e-04,\n",
       "         4.9187e-06, 1.9662e-03, 1.2679e-09],\n",
       "        [1.7271e-05, 1.6287e-06, 4.2345e-06, 2.1188e-08, 1.6752e-03, 1.4320e-11,\n",
       "         6.8661e-10, 4.4821e-09, 3.7070e-07, 2.1476e-11, 9.9776e-01, 2.3028e-11,\n",
       "         4.3206e-15, 4.0249e-12, 1.0713e-10, 1.2815e-06, 5.9534e-11, 2.8331e-09,\n",
       "         4.7368e-08, 2.3890e-08, 4.3547e-10, 2.2365e-05, 4.5724e-04, 1.8460e-05,\n",
       "         1.9477e-10, 3.5978e-05, 7.6125e-07],\n",
       "        [4.7008e-09, 8.8714e-06, 3.3168e-01, 1.2584e-05, 2.3167e-09, 1.2796e-10,\n",
       "         8.3215e-11, 6.4294e-07, 5.1951e-08, 3.3789e-06, 1.5189e-03, 6.8262e-05,\n",
       "         3.7613e-07, 6.2532e-13, 1.5927e-10, 8.2239e-06, 1.8323e-08, 1.4269e-08,\n",
       "         2.6168e-05, 8.2330e-07, 6.5794e-01, 1.8119e-11, 4.5998e-09, 2.0341e-05,\n",
       "         5.4432e-06, 8.7054e-03, 6.0415e-12],\n",
       "        [7.6217e-11, 1.0297e-06, 8.9310e-01, 7.6291e-06, 4.9770e-11, 1.1268e-11,\n",
       "         2.3960e-12, 6.5070e-08, 1.4838e-09, 9.7236e-08, 8.7060e-06, 3.8469e-05,\n",
       "         5.3698e-08, 2.8388e-14, 4.7551e-11, 1.4224e-06, 4.0301e-10, 1.7087e-10,\n",
       "         7.3744e-07, 9.8017e-08, 1.0600e-01, 1.6445e-12, 3.5969e-10, 9.5105e-07,\n",
       "         2.6780e-06, 8.3988e-04, 1.0141e-12],\n",
       "        [1.3760e-10, 2.0169e-06, 7.5529e-10, 4.6203e-10, 3.6528e-06, 6.2569e-14,\n",
       "         1.8841e-12, 3.3060e-11, 3.5996e-11, 2.2024e-06, 9.9998e-01, 1.4532e-11,\n",
       "         2.0162e-13, 1.2283e-13, 1.1489e-11, 4.1926e-07, 1.4622e-06, 3.4647e-06,\n",
       "         7.3725e-08, 1.5917e-10, 7.8053e-07, 3.2354e-09, 1.0719e-10, 8.9422e-09,\n",
       "         2.1797e-10, 5.6518e-06, 1.4759e-16],\n",
       "        [1.3104e-04, 4.9907e-02, 6.5201e-09, 1.0448e-08, 4.6991e-06, 9.5467e-09,\n",
       "         1.8849e-03, 1.4143e-09, 5.5094e-06, 2.8328e-10, 9.3457e-01, 6.1485e-06,\n",
       "         4.5709e-04, 1.6280e-10, 1.5896e-11, 8.4598e-09, 8.9167e-03, 1.1476e-05,\n",
       "         5.5373e-04, 1.0744e-06, 1.1187e-03, 2.3296e-03, 6.6407e-05, 3.3939e-05,\n",
       "         7.0014e-08, 3.5039e-07, 1.1341e-11],\n",
       "        [1.0760e-09, 2.6009e-10, 9.9912e-01, 5.5885e-05, 1.1387e-07, 2.8685e-12,\n",
       "         1.1345e-05, 5.0439e-08, 7.2913e-09, 1.6097e-14, 9.0499e-09, 2.5581e-06,\n",
       "         1.1605e-09, 1.6933e-16, 1.6559e-08, 1.4542e-08, 1.7419e-10, 8.6766e-13,\n",
       "         1.8155e-12, 1.5429e-09, 8.1137e-08, 1.1522e-16, 7.7763e-04, 5.3039e-08,\n",
       "         3.5451e-08, 3.3353e-05, 3.9086e-08],\n",
       "        [6.4407e-05, 1.1027e-08, 7.2760e-04, 2.1558e-04, 2.2844e-07, 7.1342e-07,\n",
       "         5.6229e-10, 5.3060e-06, 2.8973e-02, 9.5157e-02, 3.5431e-01, 7.9989e-07,\n",
       "         1.1812e-08, 5.7405e-05, 3.0525e-10, 9.8205e-04, 1.3539e-13, 9.6464e-08,\n",
       "         5.1906e-01, 1.8821e-06, 1.1432e-06, 1.8102e-06, 5.8482e-06, 2.9434e-04,\n",
       "         3.2505e-09, 5.2372e-05, 9.1609e-05],\n",
       "        [3.7936e-11, 6.8762e-09, 2.1563e-09, 6.7984e-06, 3.6956e-11, 9.2456e-12,\n",
       "         2.1200e-06, 6.3808e-04, 9.7454e-04, 8.2041e-08, 3.6825e-03, 1.4684e-06,\n",
       "         2.2188e-10, 2.0004e-14, 8.2786e-06, 1.0102e-06, 1.7008e-13, 2.4587e-10,\n",
       "         8.1171e-04, 9.9297e-01, 8.2386e-04, 1.0473e-11, 3.9194e-08, 8.3417e-05,\n",
       "         3.2951e-07, 4.6981e-10, 7.9566e-10],\n",
       "        [8.7935e-08, 9.8056e-06, 3.1798e-02, 4.2655e-08, 8.8204e-08, 2.7980e-05,\n",
       "         1.7122e-05, 1.8028e-07, 7.5661e-06, 2.0326e-08, 5.1588e-06, 8.5833e-07,\n",
       "         5.5780e-13, 9.5178e-15, 8.2485e-03, 6.0102e-05, 3.5242e-14, 4.2538e-09,\n",
       "         1.4210e-09, 8.3361e-01, 1.2083e-11, 1.0313e-04, 1.3740e-05, 1.2600e-01,\n",
       "         1.5118e-09, 1.6698e-08, 9.8214e-05],\n",
       "        [2.4963e-05, 8.9265e-05, 1.9430e-06, 1.2582e-08, 1.1666e-04, 1.0352e-10,\n",
       "         4.6007e-07, 1.7688e-03, 3.7458e-05, 2.9791e-05, 2.0902e-01, 5.1261e-07,\n",
       "         2.9212e-12, 2.3365e-13, 2.9768e-08, 3.3745e-07, 2.7345e-09, 6.4475e-07,\n",
       "         8.7743e-05, 6.8000e-01, 2.2002e-07, 2.3663e-05, 8.4419e-04, 1.0794e-01,\n",
       "         3.4734e-07, 1.6767e-06, 3.9307e-09],\n",
       "        [4.7008e-09, 8.8714e-06, 3.3168e-01, 1.2584e-05, 2.3167e-09, 1.2796e-10,\n",
       "         8.3215e-11, 6.4294e-07, 5.1951e-08, 3.3789e-06, 1.5189e-03, 6.8262e-05,\n",
       "         3.7613e-07, 6.2532e-13, 1.5927e-10, 8.2239e-06, 1.8323e-08, 1.4269e-08,\n",
       "         2.6168e-05, 8.2330e-07, 6.5794e-01, 1.8119e-11, 4.5998e-09, 2.0341e-05,\n",
       "         5.4432e-06, 8.7054e-03, 6.0415e-12],\n",
       "        [2.6158e-11, 2.8498e-07, 3.5812e-11, 9.9457e-11, 1.1659e-06, 2.4092e-15,\n",
       "         1.8987e-12, 3.1862e-11, 2.9332e-11, 1.3836e-08, 1.0000e+00, 1.6192e-12,\n",
       "         4.6788e-14, 2.6538e-15, 1.1232e-12, 1.3454e-08, 1.3932e-06, 1.6011e-07,\n",
       "         4.3193e-09, 3.6640e-11, 7.7494e-08, 2.9217e-10, 3.6694e-10, 8.8302e-10,\n",
       "         1.3740e-11, 1.1246e-06, 1.1155e-17],\n",
       "        [1.6026e-06, 8.9637e-05, 1.8526e-05, 1.1839e-06, 1.1439e-12, 1.9461e-10,\n",
       "         5.6787e-05, 3.5705e-11, 3.8436e-07, 2.4882e-15, 1.8209e-10, 1.6475e-01,\n",
       "         1.3222e-04, 2.8169e-12, 2.7015e-11, 1.5978e-09, 1.2808e-08, 7.7226e-11,\n",
       "         5.9078e-10, 3.1288e-05, 8.3491e-01, 3.7041e-12, 8.6239e-06, 2.1501e-10,\n",
       "         9.5938e-11, 3.8925e-08, 3.7028e-12],\n",
       "        [2.7349e-09, 1.3417e-09, 3.1273e-06, 9.4606e-05, 3.5293e-07, 2.2429e-08,\n",
       "         3.1944e-10, 1.2725e-05, 2.1678e-03, 7.5501e-04, 9.8836e-01, 1.7611e-07,\n",
       "         5.1548e-11, 2.0471e-08, 1.9445e-11, 7.9617e-03, 4.2252e-14, 6.1817e-09,\n",
       "         4.0545e-05, 4.3539e-07, 4.9466e-07, 3.2226e-11, 5.7562e-04, 1.2982e-06,\n",
       "         1.2671e-09, 2.3397e-05, 4.6895e-06],\n",
       "        [2.6624e-03, 4.8279e-04, 4.8340e-10, 4.6234e-10, 4.8669e-05, 1.4288e-13,\n",
       "         4.8646e-06, 1.2727e-03, 1.6226e-07, 1.0823e-03, 9.2356e-01, 2.5238e-09,\n",
       "         1.8044e-10, 1.1181e-13, 1.3160e-03, 2.0417e-07, 2.7649e-03, 3.8399e-05,\n",
       "         1.7065e-03, 5.6785e-02, 2.1169e-03, 4.9876e-06, 2.5492e-03, 1.9241e-04,\n",
       "         2.6241e-04, 3.1517e-03, 1.8064e-12],\n",
       "        [3.8098e-05, 1.0947e-04, 2.1588e-03, 5.1349e-08, 5.7730e-09, 9.6440e-07,\n",
       "         1.2225e-03, 5.1950e-13, 2.2608e-09, 7.8879e-14, 3.6849e-06, 4.0703e-07,\n",
       "         1.7401e-11, 1.4995e-12, 1.8364e-07, 2.2345e-10, 1.3466e-01, 1.2292e-10,\n",
       "         2.1395e-10, 2.0261e-05, 6.5461e-10, 2.9377e-05, 8.6052e-01, 1.1573e-03,\n",
       "         1.4038e-07, 7.4229e-05, 6.0009e-06],\n",
       "        [1.4606e-09, 3.2662e-08, 1.7357e-08, 7.9820e-06, 8.7722e-04, 2.5674e-10,\n",
       "         5.5043e-10, 1.7828e-10, 4.5060e-08, 2.0809e-08, 9.9911e-01, 1.8833e-11,\n",
       "         3.0023e-11, 5.9058e-10, 3.1085e-09, 4.5916e-08, 2.7128e-11, 7.7988e-09,\n",
       "         3.9517e-09, 2.0784e-12, 4.0790e-08, 1.8654e-10, 2.7114e-06, 1.6404e-09,\n",
       "         3.8802e-10, 1.6814e-07, 2.9888e-10]])"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob = counts / counts.sum(1, keepdims=True)\n",
    "prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "fd37882c-eebf-49b3-9593-85173a3d369b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 27])"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "16e6d55f-1ae9-46a8-9614-d1a2b2a64f0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
       "        18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.arange(32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "6ca5abe1-849c-4bd0-8cdd-f78f84078fd8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 5, 13, 13,  1,  0, 15, 12,  9, 22,  9,  1,  0,  1, 22,  1,  0,  9, 19,\n",
       "         1,  2,  5, 12, 12,  1,  0, 19, 15, 16,  8,  9,  1,  0])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "aff5ea41-f4ad-4ad3-8134-2cf81a8cc116",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.2796e-10, 2.4573e-08, 1.9857e-14, 6.2196e-10, 8.5623e-13, 8.2239e-06,\n",
       "        1.1673e-07, 7.1976e-03, 2.3370e-09, 1.4973e-07, 2.1407e-06, 1.3090e-08,\n",
       "        8.8714e-06, 2.6627e-10, 5.4176e-06, 1.7271e-05, 3.3789e-06, 9.8017e-08,\n",
       "        2.0169e-06, 6.5201e-09, 2.8685e-12, 1.1812e-08, 2.2188e-10, 9.8056e-06,\n",
       "        2.4963e-05, 8.2330e-07, 1.3454e-08, 1.2808e-08, 2.1678e-03, 1.0823e-03,\n",
       "        1.0947e-04, 1.4606e-09])"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob[torch.arange(32),Y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "3f33a614-5242-4e7e-8402-5c7189a7d22d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(16.2385)"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = -prob[torch.arange(32), Y].log().mean()\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "70a7db45-a990-409e-be25-146d9fe45613",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 3]), torch.Size([32]))"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "id": "5398c0c6-3375-4bb5-a8a7-cbf1c10cf404",
   "metadata": {},
   "outputs": [],
   "source": [
    "#All together now\n",
    "CONTEXT_LENGTH = 3\n",
    "EMB_VECTOR_DIMS = 15\n",
    "EMBEDDING_DIMS = EMB_VECTOR_DIMS * CONTEXT_LENGTH\n",
    "C = torch.randn((27,EMB_VECTOR_DIMS))\n",
    "W1 = torch.randn((EMBEDDING_DIMS,200))\n",
    "b1 = torch.randn(200)\n",
    "W2 = torch.randn((200,27))\n",
    "b2 = torch.randn(27)\n",
    "parameters = [C, W1, b1, W2, b2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "8cd69b84-cd72-436c-a7d0-921bdc7f632a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11897"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(p.nelement() for p in parameters) # total # of parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "id": "75aa8c28-b96e-442f-b179-c4b50b54351e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in parameters:\n",
    "    p.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "id": "0973634c-d1c9-4f33-b04a-96eadb5716a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.9292185306549072\n"
     ]
    }
   ],
   "source": [
    "#NOTE: The dimensions (32 specifically) were with only 5 training examples. Wherever you see 32, it's actually just the # of training examples.\n",
    "for _ in range(20000):\n",
    "\n",
    "    #mini-batch of size 32\n",
    "    ix = torch.randint(0, Xtr.shape[0], (32, )) #returns the indexes of 32 samples from our train set\n",
    "    \n",
    "    #forward pass\n",
    "    emb = C[Xtr[ix]] # (32, 3, 2)\n",
    "    h = torch.tanh(emb.view(emb.shape[0], EMBEDDING_DIMS) @ W1 + b1) # (32, 100), first layer\n",
    "    logits = h @ W2 + b2 # (32, 27), output/second layer\n",
    "    # counts = logits.exp()\n",
    "    # prob = counts / counts.sum(1, keepdims= True)\n",
    "    # loss = -prob[torch.arange(32), Y].log().mean()\n",
    "    loss = F.cross_entropy(logits, Ytr[ix]) # this is better than the three lines above\n",
    "    # print(loss.item())\n",
    "    \n",
    "    #backward pass\n",
    "    for p in parameters:\n",
    "        p.grad = None\n",
    "    loss.backward()\n",
    "    \n",
    "    #update\n",
    "    for p in parameters:\n",
    "        p.data += -0.01 * p.grad\n",
    "\n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "c0d1456a-2339-4888-814d-efce146f349f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([182691, 3]) torch.Size([182691])\n",
      "torch.Size([22793, 3]) torch.Size([22793])\n",
      "torch.Size([22662, 3]) torch.Size([22662])\n"
     ]
    }
   ],
   "source": [
    "#training, val/dev split, test split\n",
    "#.8, .1, .1\n",
    "# NOTE ON SPLITS: Validation split is for hyperparameter tuning. Test split should only be evaluated on a few times to avoid the model beginning\n",
    "#to overfit the test set as well.\n",
    "\n",
    "# build the dataset\n",
    "def build_dataset(words):\n",
    "    block_size = 3 #context length\n",
    "    X, Y = [], []\n",
    "    for w in words:\n",
    "        # print(w)\n",
    "        context = [0] * block_size #starts as block_size #'.'s. (Since itos[0] is . and we use the integer representation of the character)\n",
    "        for ch in w + '.':\n",
    "            ix = stoi[ch]\n",
    "            X.append(context)\n",
    "            Y.append(ix)\n",
    "            # print(''.join(itos[i] for i in context), '---->', itos[ix])\n",
    "            context = context[1:] + [ix]\n",
    "    \n",
    "    X = torch.tensor(X)\n",
    "    Y = torch.tensor(Y)\n",
    "    print(X.shape, Y.shape)\n",
    "    return X, Y\n",
    "\n",
    "import random\n",
    "random.seed(42)\n",
    "random.shuffle(words)\n",
    "\n",
    "n1 = int(0.8 * len(words))\n",
    "n2 = int(0.9 * len(words))\n",
    "Xtr, Ytr = build_dataset(words[:n1])\n",
    "Xdev, Ydev = build_dataset(words[n1:n2])\n",
    "Xte, Yte = build_dataset(words[n2:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "e5579030-a503-46df-a55f-6315f493a6cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32033"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "id": "59bc4f06-7a09-4326-8ed7-82450ec924fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.1579, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 358,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Evaluation\n",
    "emb = C[Xdev]\n",
    "h = torch.tanh(emb.view(emb.shape[0], EMBEDDING_DIMS) @ W1 + b1)\n",
    "logits = h @ W2 + b2\n",
    "loss = F.cross_entropy(logits, Ydev)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "63cf1090-c314-405a-984c-592c7d462634",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n'"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#NOTE ON TRAINING AND EVALUATION\n",
    "\"\"\"\n",
    "If we are underfitting, we will have a very similar loss on the training as we do on the eval and test sets. This suggests that our model is very \n",
    "small and we will increase performance by increasing the size.\n",
    "Large nets take longer to converge.\n",
    "Batches not only decrease training time, but also can lead to better gradients as stochastic GD leads to a lot of noise in training as we're calculating\n",
    "gradients at every piece of training data.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "4cfc3928-c4e0-4ed4-bda3-c3275ccadb0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([156061, 163476, 205784,  89185, 191318,  51782,  61629, 174337, 167960,\n",
       "         80483,  26513, 160966, 149353, 109253, 105360, 135393, 169759,  68677,\n",
       "         27572, 218261, 220674, 156892,  73078,  61898, 129056,  54335, 224361,\n",
       "         48185,   6119, 186061, 160520,  90039])"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.randint(0, X.shape[0], (32, ))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
